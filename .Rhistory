data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = "1",           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
# Set seed para reproducibilidade
set.seed(1234)
# k-fold cross validation
nround.cv = 200
system.time( bst.cv <- xgb.cv(param = param,
data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = 10,           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
# Set seed para reproducibilidade
set.seed(1234)
# k-fold cross validation
nround.cv = 200
system.time( bst.cv <- xgb.cv(param = param,
data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = 0,           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
# Set seed para reproducibilidade
set.seed(1234)
# k-fold cross validation
nround.cv = 200
system.time( bst.cv <- xgb.cv(param = param,
data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = 1,           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
# Set seed para reproducibilidade
set.seed(1234)
# k-fold cross validation
nround.cv = 200
system.time( bst.cv <- xgb.cv(param = param,
data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = 2000,           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
# Set seed para reproducibilidade
set.seed(1234)
# k-fold cross validation
nround.cv = 200
system.time( bst.cv <- xgb.cv(param = param,
data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = 110,           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
# Set seed para reproducibilidade
set.seed(1234)
# k-fold cross validation
nround.cv = 200
system.time( bst.cv <- xgb.cv(param = param,
data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = 109,           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
# Set seed para reproducibilidade
set.seed(1234)
# k-fold cross validation
nround.cv = 200
system.time( bst.cv <- xgb.cv(param = param,
data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = default,           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
# Set seed para reproducibilidade
set.seed(1234)
# k-fold cross validation
nround.cv = 200
system.time( bst.cv <- xgb.cv(param = param,
data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = num_range(),           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
# Set seed para reproducibilidade
set.seed(1234)
# k-fold cross validation
nround.cv = 200
system.time( bst.cv <- xgb.cv(param = param,
data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = numeric(),           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
# Set seed para reproducibilidade
set.seed(1234)
# k-fold cross validation
nround.cv = 200
system.time( bst.cv <- xgb.cv(param = param,
data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = names(train$NU_NOTA_MT),           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
# Set seed para reproducibilidade
set.seed(1234)
# k-fold cross validation
nround.cv = 200
system.time( bst.cv <- xgb.cv(param = param,
data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = length(names(train$NU_NOTA_MT)),           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
# Set seed para reproducibilidade
set.seed(1234)
# k-fold cross validation
nround.cv = 200
system.time( bst.cv <- xgb.cv(param = param,
data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
length(names(train$NU_NOTA_MT))
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = length((train$NU_NOTA_MT)),           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
# Set seed para reproducibilidade
set.seed(1234)
# k-fold cross validation
nround.cv = 200
system.time( bst.cv <- xgb.cv(param = param,
data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
length((train$NU_NOTA_MT))
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = length(table(train$NU_NOTA_MT)),           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
# Set seed para reproducibilidade
set.seed(1234)
# k-fold cross validation
nround.cv = 200
system.time( bst.cv <- xgb.cv(param = param,
data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
length(table(train$NU_NOTA_MT))
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = num.class,           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
length(levels(outcome))
num.class = length((outcome))
num.class
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = num.class,           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
# Set seed para reproducibilidade
set.seed(1234)
# k-fold cross validation
nround.cv = 200
system.time( bst.cv <- xgb.cv(param = param,
data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
num.class
num.class = length(table(outcome))
num.class
train = treinando
test = testando
# Visualizando as dimensões
dim(train)
dim(test)
names(train)
# Variável de saída
outcome.org = train[, "NU_NOTA_MT"]
outcome = outcome.org
head(outcome)
num.class = length(table(outcome))
train$classe = NULL
# Checando zero variância
zero.var = nearZeroVar(train, saveMetrics = TRUE)
zero.var
# t-Distributed Stochastic Neighbor Embedding
tsne = Rtsne(as.matrix(train),
check_duplicates = FALSE,
pca = TRUE,
perplexity = 30,
theta = 0.5,
dims = 2)
# Preparando as variáveis para o Plot
embedding = as.data.frame(tsne$Y)
embedding$Class = outcome.org
# Visualização o Plot com ggplot
g = ggplot(embedding, aes(x = V1, y = V2, color = Class)) +
geom_point(size = 1.25) +
guides(colour = guide_legend(override.aes = list(size = 6))) +
xlab("") + ylab("") +
ggtitle("t-SNE 2D Embedding of 'Classe' Outcome") +
theme_light(base_size = 20) +
theme(axis.text.x = element_blank(),
axis.text.y = element_blank())
print(g)
# Convertendo os dados em Mztriz
train.matrix = as.matrix(train)
mode(train.matrix) = "numeric"
test.matrix = as.matrix(test)
mode(test.matrix) = "numeric"
# Convertendo de fator para Matriz
y = as.matrix(as.integer(outcome)-1)
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = num.class,           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
# Set seed para reproducibilidade
set.seed(1234)
# k-fold cross validation
nround.cv = 200
system.time( bst.cv <- xgb.cv(param = param,
data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
1:num.class
num.class 1:num.class
num.class = 1:num.class
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = num.class,           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
# Set seed para reproducibilidade
set.seed(1234)
# k-fold cross validation
nround.cv = 200
system.time( bst.cv <- xgb.cv(param = param,
data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
outcome
num.class = length(table(outcome))
1:num.class
num.class = length(levels(outcome))
num.class = 1:num.class
1:num.class
num.class
# Lista de parâmetros XGBoost
param <- list("objective" = "multi:softprob",    # multiclass classification
"num_class" = num.class,           # number of classes
"eval_metric" = "merror",          # evaluation metric
"nthread" = 8,                     # number of threads to be used
"max_depth" = 16,                  # maximum depth of tree
"eta" = 0.3,                       # step size shrinkage
"gamma" = 0,                       # minimum loss reduction
"subsample" = 1,                   # part of data instances to grow tree
"colsample_bytree" = 1,            # subsample ratio of columns when constructing each tree
"min_child_weight" = 12            # minimum sum of instance weight needed in a child
)
# Set seed para reproducibilidade
set.seed(1234)
# k-fold cross validation
nround.cv = 200
system.time( bst.cv <- xgb.cv(param = param,
data = train.matrix,
label = y,
nfold = 4,
nrounds = nround.cv,
prediction = TRUE,
verbose = FALSE) )
num.class = 1:num.class
# Visualizando as dimensões
dim(train)
dim(test)
names(test)
# Variável de saída
outcome.org = train[, "classe"]
outcome = outcome.org
# Variável de saída
outcome.org = train[, "NU_NOTA_MT"]
outcome = outcome.org
levels(outcome)
table(outcome)
length(levels(outcome)
length(levels(outcome))
length(levels(outcome))
source('C:/Users/Matilde/Dropbox/DESAFIO-Codenation(ENEM)/desafio_nota_do_enem/PrevendoNotaDeMatematica.r', encoding = 'UTF-8', echo=TRUE)
length(table(outcome))
length(table(outcome))
