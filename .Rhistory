table(regioes)
notateste$regioes = regioes
table(notateste$regioes)
names(table(notateste$regioes))
#Transformar os dois datasets com as mesmas variáveis
notateste = merge(notateste,SOMA_PIB_2016)
nomes <- names(notateste)
# Criar regioes com os estados por prova
glimpse(nota$SG_UF_RESIDENCIA)
table(nota$SG_UF_RESIDENCIA)
x <- c(nota$SG_UF_RESIDENCIA)
lookup <- c( AC = "NORTE",  AL = "NORDESTE",  AM = "NORTE",  AP = "NORTE",  BA = "NORDESTE", CE = "NORDESTE", DF = "CENTRO-OESTE", ES = "SUDESTE", GO = "CENTRO-OESTE", MA = "NORDESTE", MG = "SUDESTE", MS = "CENTRO-OESTE", MT = "CENTRO-OESTE", PA = "NORTE", PB = "NORDESTE", PE = "NORDESTE", PI = "NORDESTE", PR = "SUL", RJ = "SUDESTE", RN = "NORDESTE", RO = "NORTE", RR = "NORTE", RS = "SUL", SC = "SUL", SE = "NORDESTE", SP = "SUDESTE", TO = "NORTE")
uniao = lookup[x]
regioes = unname(uniao)
table(regioes)
nota$regioes = regioes
table(nota$regioes)
names(table(nota$regioes))
#Transformar os dois datasets com as mesmas variáveis
nota = merge(nota,SOMA_PIB_2016)
# ****************************************************
# ***                   TREINO                     ***
# ***                                              ***
# ****************************************************
# Segmentando apenas os dados que temos no teste
treinando = nota[c(nomes, "NU_NOTA_MT")]
treinando$Q027 = as.numeric(treinando$Q027)
summary(treinando$Q027)
treinando$Q027[(treinando$Q027 == 1)] <- 1
glimpse(treinando)
summary(treinando$Q027)
#treinando$TP_DEPENDENCIA_ADM_ESC[(is.na(treinando$TP_DEPENDENCIA_ADM_ESC))] <- 2.26
treinando$TP_ENSINO = NULL
treinando$TP_DEPENDENCIA_ADM_ESC = NULL
#Para que eu possa pegar exatamente os nomes das variáveis mais a target
treinando = treinando %>% drop_na()
any(is.na(treinando))
# ****************************************************
# ***              FORMATO NUMÉRICO                ***
# ****************************************************
glimpse(treinando)
#Transformar para numérico
treinando$SG_UF_RESIDENCIA = as.numeric(treinando$SG_UF_RESIDENCIA)
treinando$TP_SEXO = as.numeric(treinando$TP_SEXO)
treinando$Q001 = as.numeric(treinando$Q001)
treinando$Q002 = as.numeric(treinando$Q002)
treinando$Q006 = as.numeric(treinando$Q006)
treinando$Q024 = as.numeric(treinando$Q024)
treinando$Q025 = as.numeric(treinando$Q025)
treinando$Q026 = as.numeric(treinando$Q026)
treinando$Q047 = as.numeric(treinando$Q047)
treinando$CO_PROVA_CN = as.numeric(treinando$CO_PROVA_CN)
treinando$CO_PROVA_CH = as.numeric(treinando$CO_PROVA_CH)
treinando$CO_PROVA_LC = as.numeric(treinando$CO_PROVA_LC)
treinando$CO_PROVA_MT = as.numeric(treinando$CO_PROVA_MT)
treinando$regioes = as.factor(treinando$regioes)
treinando$regioes = as.numeric(treinando$regioes)
treinando$NU_INSCRICAO = NULL
glimpse(treinando)
# Criar variável de segurança
testando = notateste
testando$Q027 = as.numeric(testando$Q027)
testando$Q027[(treinando$Q027 == 1)] <- mean(testando$Q027)
#testando$Q027 = NULL
testando$TP_DEPENDENCIA_ADM_ESC = NULL
testando$TP_ENSINO = NULL
## LIMPEZA Retirando valores NA da tabela
testando = testando %>% drop_na()
any(is.na(testando))
# Transformar variaveis em numeric
testando$SG_UF_RESIDENCIA = as.numeric(testando$SG_UF_RESIDENCIA)
testando$TP_SEXO = as.numeric(testando$TP_SEXO)
testando$Q001 = as.numeric(testando$Q001)
testando$Q002 = as.numeric(testando$Q002)
testando$Q006 = as.numeric(testando$Q006)
testando$Q024 = as.numeric(testando$Q024)
testando$Q025 = as.numeric(testando$Q025)
testando$Q026 = as.numeric(testando$Q026)
testando$Q047 = as.numeric(testando$Q047)
testando$CO_PROVA_CN = as.numeric(testando$CO_PROVA_CN)
testando$CO_PROVA_CH = as.numeric(testando$CO_PROVA_CH)
testando$CO_PROVA_LC = as.numeric(testando$CO_PROVA_LC)
testando$CO_PROVA_MT = as.numeric(testando$CO_PROVA_MT)
testando$regioes = as.factor(testando$regioes)
testando$regioes = as.numeric(testando$regioes)
notateste = testando
testando$NU_INSCRICAO = NULL
# Vizualizar se os dados estão em estado numérico para envolver no algoritmo
glimpse(testando)
##Regressão linear múltipla
# Etapa 3: Treinando o Modelo (usando os dados de treino)
modelo <- lm(NU_NOTA_MT ~ ., data = treinando)
# Visualizando os coeficientes
modelo
previsao1 <- predict(modelo, testando)
View(previsao1)
# Etapa 4: Avaliando a Performance do Modelo
# Mais detalhes sobre o modelo
summary(modelo)
#### RANDONFOREST (Arvores de decisão) ####
trainset = treinando
testset = testando
modelo_rf_v1 = rpart(NU_NOTA_MT ~ ., data = trainset, control = rpart.control( cp = .000999999999999999))
summary(modelo_rf_v1)
# Previsões nos dados de teste
tree_pred = predict(modelo_rf_v1, testset)
enviotree = data.frame(notateste$NU_INSCRICAO)
colnames(enviotree) = c("NU_INSCRICAO")
enviotree$NU_NOTA_MT = tree_pred
write.csv(enviotree, "answer.csv", row.names = FALSE)
summary(treinando$TP_DEPENDENCIA_ADM_ESC)
# Machine Learning - Regressão
# Prevendo Notas de Matemática dos Participantes do ENEM
# Configurando o diretório de trabalho
setwd("C:/Users/Matilde/Dropbox/DESAFIO-Codenation(ENEM)/desafio_nota_do_enem")
getwd()
library(dplyr)     # Filtragens
library(psych)     # Scatterplot Matrix
library(e1071)     # SVM
library(rpart)     # RANDON FOREST
library(readr)     # GERAR CSV
library(tidyverse) # Drop_NA()
library(readxl)    # Importar xls
# ****************************************************
# ***                   DADOS                      ***
# ***                                              ***
# ****************************************************
# Etapa 1 - Coletando os dados
nota <- read.csv("train.csv")
notateste <- read.csv("test.csv")
PIB_2016 <- read_excel("PIB-2016.xls")
TP_DEPENDENCIA_ADM_ESC
# ****************************************************
# ***                   INCLUIR                    ***
# ***                NOVAS VARIÁVEIS               ***
# ****************************************************
#média do pib por estado
PIB_2016 = PIB_2016 %>% filter(Ano == 2016)
PIB_2016 = PIB_2016[c(names(PIB_2016)[5], names(PIB_2016)[42])]
SOMA_PIB_2016 = aggregate(PIB_2016$`Produto Interno Bruto per capita
(R$ 1,00)`,
by=list(PIB_2016$`Sigla da Unidade da Federação`),
FUN = sum)
colnames(SOMA_PIB_2016) = c("SG_UF_RESIDENCIA","media")
lookup <- c( AC = "13.751",  AL = "49.456",  AP = "14.339",  AM = "89.017 ",  BA = "258.649", CE = "138.379", DF = "235.497", ES = "109.227", GO = "181.692", MA = "85.286", MT = "123.834", MS = "91.866", MG = "544.634", PR = "401.662", PB = "59.089", PA = "138.068", PE = "167.290", PI = "41.406", RJ = "640.186", RN = "59.661", RS = "408.645", RO = "39.451", RR = "11.011", SC = "256.661", SE = "38.867", SP = "2.038.005", TO = "31.576")
SOMA_PIB_2016$media = lookup
# Criar regioes com os estados por prova
glimpse(notateste$SG_UF_RESIDENCIA)
table(notateste$SG_UF_RESIDENCIA)
x <- c(notateste$SG_UF_RESIDENCIA)
lookup <- c( AC = "NORTE",  AL = "NORDESTE",  AM = "NORTE",  AP = "NORTE",  BA = "NORDESTE", CE = "NORDESTE", DF = "CENTRO-OESTE", ES = "SUDESTE", GO = "CENTRO-OESTE", MA = "NORDESTE", MG = "SUDESTE", MS = "CENTRO-OESTE", MT = "CENTRO-OESTE", PA = "NORTE", PB = "NORDESTE", PE = "NORDESTE", PI = "NORDESTE", PR = "SUL", RJ = "SUDESTE", RN = "NORDESTE", RO = "NORTE", RR = "NORTE", RS = "SUL", SC = "SUL", SE = "NORDESTE", SP = "SUDESTE", TO = "NORTE")
uniao = lookup[x]
regioes = unname(uniao)
table(regioes)
notateste$regioes = regioes
table(notateste$regioes)
names(table(notateste$regioes))
#Transformar os dois datasets com as mesmas variáveis
notateste = merge(notateste,SOMA_PIB_2016)
nomes <- names(notateste)
# Criar regioes com os estados por prova
glimpse(nota$SG_UF_RESIDENCIA)
table(nota$SG_UF_RESIDENCIA)
x <- c(nota$SG_UF_RESIDENCIA)
lookup <- c( AC = "NORTE",  AL = "NORDESTE",  AM = "NORTE",  AP = "NORTE",  BA = "NORDESTE", CE = "NORDESTE", DF = "CENTRO-OESTE", ES = "SUDESTE", GO = "CENTRO-OESTE", MA = "NORDESTE", MG = "SUDESTE", MS = "CENTRO-OESTE", MT = "CENTRO-OESTE", PA = "NORTE", PB = "NORDESTE", PE = "NORDESTE", PI = "NORDESTE", PR = "SUL", RJ = "SUDESTE", RN = "NORDESTE", RO = "NORTE", RR = "NORTE", RS = "SUL", SC = "SUL", SE = "NORDESTE", SP = "SUDESTE", TO = "NORTE")
uniao = lookup[x]
regioes = unname(uniao)
table(regioes)
nota$regioes = regioes
table(nota$regioes)
names(table(nota$regioes))
#Transformar os dois datasets com as mesmas variáveis
nota = merge(nota,SOMA_PIB_2016)
# ****************************************************
# ***                   TREINO                     ***
# ***                                              ***
# ****************************************************
# Segmentando apenas os dados que temos no teste
treinando = nota[c(nomes, "NU_NOTA_MT")]
treinando$Q027 = as.numeric(treinando$Q027)
treinando$Q027[(treinando$Q027 == 1)] <- mean(treinando$Q027)
glimpse(treinando)
summary(treinando$TP_DEPENDENCIA_ADM_ESC)
install.packages("catboost")
install.packages('devtools')
conda config --add channels conda-forge
library(devtools)
library(catboost)  # Algoritmo de boost
library(devtools)  # Github
devtools::install_url('BINARY_URL'[, INSTALL_opts|args = c("--no-multiarch")])# ****************************************************
devtools::install_url('BINARY_URL'[, INSTALL_opts|args = c("--no-multiarch")])
devtools::install_url('https://github.com/catboost/catboost/releases/download/v0.6.1.1/catboost-R-Windows-0.6.1.1.tgz', args = c("--no-multiarch"))
library(catboost)  # Algoritmo de boost
devtools::install_github('catboost/catboost', subdir = 'catboost/R-package')
library(catboost)  # Algoritmo de boost
##CATBOOST
dataset = treinando
label_values = c(0,1,1)
fit_params <- list(iterations = 100,
loss_function = 'Logloss',
ignored_features = c(4,9),
border_count = 32,
depth = 5,
learning_rate = 0.03,
l2_leaf_reg = 3.5,
task_type = 'GPU')
pool = catboost.load_pool(dataset, label = label_values, cat_features = c(0,3))
glimpse(treinando)
# Machine Learning - Regressão
# Prevendo Notas de Matemática dos Participantes do ENEM
# Configurando o diretório de trabalho
setwd("C:/Users/Matilde/Dropbox/DESAFIO-Codenation(ENEM)/desafio_nota_do_enem")
getwd()
library(dplyr)     # Filtragens
library(psych)     # Scatterplot Matrix
library(e1071)     # SVM
library(rpart)     # RANDON FOREST
library(readr)     # GERAR CSV
library(tidyverse) # Drop_NA()
library(readxl)    # Importar xls
library(catboost)  # Algoritmo de boost
library(devtools)  # Github
#devtools::install_github('catboost/catboost', subdir = 'catboost/R-package')
# ****************************************************
# ***                   DADOS                      ***
# ***                                              ***
# ****************************************************
# Etapa 1 - Coletando os dados
nota <- read.csv("train.csv")
notateste <- read.csv("test.csv")
PIB_2016 <- read_excel("PIB-2016.xls")
TP_DEPENDENCIA_ADM_ESC
# ****************************************************
# ***                   INCLUIR                    ***
# ***                NOVAS VARIÁVEIS               ***
# ****************************************************
#média do pib por estado
PIB_2016 = PIB_2016 %>% filter(Ano == 2016)
PIB_2016 = PIB_2016[c(names(PIB_2016)[5], names(PIB_2016)[42])]
SOMA_PIB_2016 = aggregate(PIB_2016$`Produto Interno Bruto per capita
(R$ 1,00)`,
by=list(PIB_2016$`Sigla da Unidade da Federação`),
FUN = sum)
colnames(SOMA_PIB_2016) = c("SG_UF_RESIDENCIA","media")
lookup <- c( AC = "13.751",  AL = "49.456",  AP = "14.339",  AM = "89.017 ",  BA = "258.649", CE = "138.379", DF = "235.497", ES = "109.227", GO = "181.692", MA = "85.286", MT = "123.834", MS = "91.866", MG = "544.634", PR = "401.662", PB = "59.089", PA = "138.068", PE = "167.290", PI = "41.406", RJ = "640.186", RN = "59.661", RS = "408.645", RO = "39.451", RR = "11.011", SC = "256.661", SE = "38.867", SP = "2.038.005", TO = "31.576")
SOMA_PIB_2016$media = lookup
# Criar regioes com os estados por prova
glimpse(notateste$SG_UF_RESIDENCIA)
table(notateste$SG_UF_RESIDENCIA)
x <- c(notateste$SG_UF_RESIDENCIA)
lookup <- c( AC = "NORTE",  AL = "NORDESTE",  AM = "NORTE",  AP = "NORTE",  BA = "NORDESTE", CE = "NORDESTE", DF = "CENTRO-OESTE", ES = "SUDESTE", GO = "CENTRO-OESTE", MA = "NORDESTE", MG = "SUDESTE", MS = "CENTRO-OESTE", MT = "CENTRO-OESTE", PA = "NORTE", PB = "NORDESTE", PE = "NORDESTE", PI = "NORDESTE", PR = "SUL", RJ = "SUDESTE", RN = "NORDESTE", RO = "NORTE", RR = "NORTE", RS = "SUL", SC = "SUL", SE = "NORDESTE", SP = "SUDESTE", TO = "NORTE")
uniao = lookup[x]
regioes = unname(uniao)
table(regioes)
notateste$regioes = regioes
table(notateste$regioes)
names(table(notateste$regioes))
#Transformar os dois datasets com as mesmas variáveis
notateste = merge(notateste,SOMA_PIB_2016)
nomes <- names(notateste)
# Criar regioes com os estados por prova
glimpse(nota$SG_UF_RESIDENCIA)
table(nota$SG_UF_RESIDENCIA)
x <- c(nota$SG_UF_RESIDENCIA)
lookup <- c( AC = "NORTE",  AL = "NORDESTE",  AM = "NORTE",  AP = "NORTE",  BA = "NORDESTE", CE = "NORDESTE", DF = "CENTRO-OESTE", ES = "SUDESTE", GO = "CENTRO-OESTE", MA = "NORDESTE", MG = "SUDESTE", MS = "CENTRO-OESTE", MT = "CENTRO-OESTE", PA = "NORTE", PB = "NORDESTE", PE = "NORDESTE", PI = "NORDESTE", PR = "SUL", RJ = "SUDESTE", RN = "NORDESTE", RO = "NORTE", RR = "NORTE", RS = "SUL", SC = "SUL", SE = "NORDESTE", SP = "SUDESTE", TO = "NORTE")
uniao = lookup[x]
regioes = unname(uniao)
table(regioes)
nota$regioes = regioes
table(nota$regioes)
names(table(nota$regioes))
#Transformar os dois datasets com as mesmas variáveis
nota = merge(nota,SOMA_PIB_2016)
# ****************************************************
# ***                   TREINO                     ***
# ***                                              ***
# ****************************************************
# Segmentando apenas os dados que temos no teste
treinando = nota[c(nomes, "NU_NOTA_MT")]
treinando$Q027 = as.numeric(treinando$Q027)
treinando$Q027[(treinando$Q027 == 1)] <- mean(treinando$Q027)
glimpse(treinando)
#summary(treinando$TP_DEPENDENCIA_ADM_ESC)
#treinando$TP_DEPENDENCIA_ADM_ESC[(is.na(treinando$TP_DEPENDENCIA_ADM_ESC))] <- 2.26
treinando$TP_ENSINO = NULL
treinando$TP_DEPENDENCIA_ADM_ESC = NULL
#Para que eu possa pegar exatamente os nomes das variáveis mais a target
treinando = treinando %>% drop_na()
#treinando$Q027 = NULL
#treinando$TP_DEPENDENCIA_ADM_ESC = NULL
any(is.na(treinando))
# ****************************************************
# ***              FORMATO NUMÉRICO                ***
# ****************************************************
glimpse(treinando)
#Transformar para numérico
treinando$SG_UF_RESIDENCIA = as.numeric(treinando$SG_UF_RESIDENCIA)
treinando$TP_SEXO = as.numeric(treinando$TP_SEXO)
treinando$Q001 = as.numeric(treinando$Q001)
treinando$Q002 = as.numeric(treinando$Q002)
treinando$Q006 = as.numeric(treinando$Q006)
treinando$Q024 = as.numeric(treinando$Q024)
treinando$Q025 = as.numeric(treinando$Q025)
treinando$Q026 = as.numeric(treinando$Q026)
treinando$Q047 = as.numeric(treinando$Q047)
treinando$CO_PROVA_CN = as.numeric(treinando$CO_PROVA_CN)
treinando$CO_PROVA_CH = as.numeric(treinando$CO_PROVA_CH)
treinando$CO_PROVA_LC = as.numeric(treinando$CO_PROVA_LC)
treinando$CO_PROVA_MT = as.numeric(treinando$CO_PROVA_MT)
treinando$regioes = as.factor(treinando$regioes)
treinando$regioes = as.numeric(treinando$regioes)
treinando$NU_INSCRICAO = NULL
glimpse(treinando)
# ****************************************************
# ***                   TESTE                      ***
# ***                                              ***
# ****************************************************
# Criar variável de segurança
testando = notateste
testando$Q027 = as.numeric(testando$Q027)
testando$Q027[(treinando$Q027 == 1)] <- mean(testando$Q027)
#testando$Q027 = NULL
testando$TP_DEPENDENCIA_ADM_ESC = NULL
testando$TP_ENSINO = NULL
## LIMPEZA Retirando valores NA da tabela
testando = testando %>% drop_na()
any(is.na(testando))
# Transformar variaveis em numeric
testando$SG_UF_RESIDENCIA = as.numeric(testando$SG_UF_RESIDENCIA)
testando$TP_SEXO = as.numeric(testando$TP_SEXO)
testando$Q001 = as.numeric(testando$Q001)
testando$Q002 = as.numeric(testando$Q002)
testando$Q006 = as.numeric(testando$Q006)
testando$Q024 = as.numeric(testando$Q024)
testando$Q025 = as.numeric(testando$Q025)
testando$Q026 = as.numeric(testando$Q026)
testando$Q047 = as.numeric(testando$Q047)
testando$CO_PROVA_CN = as.numeric(testando$CO_PROVA_CN)
testando$CO_PROVA_CH = as.numeric(testando$CO_PROVA_CH)
testando$CO_PROVA_LC = as.numeric(testando$CO_PROVA_LC)
testando$CO_PROVA_MT = as.numeric(testando$CO_PROVA_MT)
testando$regioes = as.factor(testando$regioes)
testando$regioes = as.numeric(testando$regioes)
notateste = testando
testando$NU_INSCRICAO = NULL
# Vizualizar se os dados estão em estado numérico para envolver no algoritmo
glimpse(testando)
# ****************************************************
# ***               MODELOS DE ML                  ***
# ***                                              ***
# ****************************************************
##Regressão linear múltipla
# Etapa 3: Treinando o Modelo (usando os dados de treino)
modelo <- lm(NU_NOTA_MT ~ ., data = treinando)
# Visualizando os coeficientes
modelo
previsao1 <- predict(modelo, testando)
View(previsao1)
#treinando$Prev = previsao1
# Etapa 4: Avaliando a Performance do Modelo
# Mais detalhes sobre o modelo
summary(modelo)
####### -> 48%
# Lembre-se que correlação não implica causalidade
#### RANDONFOREST (Arvores de decisão) ####
trainset = treinando
testset = testando
modelo_rf_v1 = rpart(NU_NOTA_MT ~ ., data = trainset, control = rpart.control( cp = .000999999999999999))
summary(modelo_rf_v1)
# Previsões nos dados de teste
tree_pred = predict(modelo_rf_v1, testset)
enviotree = data.frame(notateste$NU_INSCRICAO)
colnames(enviotree) = c("NU_INSCRICAO")
enviotree$NU_NOTA_MT = tree_pred
write.csv(enviotree, "answer.csv", row.names = FALSE)
##CATBOOST
dataset = treinando
glimpse(treinando)
label_values = c(0,1,1)
fit_params <- list(iterations = 100,
loss_function = 'Logloss',
ignored_features = c(4,9),
border_count = 32,
depth = 5,
learning_rate = 0.03,
l2_leaf_reg = 3.5,
task_type = 'GPU')
pool = catboost.load_pool(dataset, label = label_values, cat_features = c(0,3))
pool = catboost.load_pool(dataset, cat_features = c(0,3))
pool = catboost.load_pool(dataset)
glimpse(treinando)
label_values = c(0,1,1)
fit_params <- list(iterations = 100,
loss_function = 'Logloss',
ignored_features = c(4,9),
border_count = 32,
depth = 5,
learning_rate = 0.03,
l2_leaf_reg = 3.5,
task_type = 'GPU')
pool = catboost.load_pool(dataset, label = label_values, cat_features = c(0,3))
catboost.load_pool <- function(dataset, label = NULL, cat_features = NULL, column_description = NULL,
pairs = NULL, delimiter = "\t", has_header = FALSE, weight = NULL,
group_id = NULL, group_weight = NULL, subgroup_id = NULL, pairs_weight = NULL,
baseline = NULL, feature_names = NULL, thread_count = -1) {
if (!is.null(pairs) && (is.character(dataset) != is.character(pairs))) {
stop("Data and pairs should be the same types.")
}
if (is.character(dataset) && length(dataset) == 1) {
for (arg in list("label", "cat_features", "weight", "group_id",
"group_weight", "subgroup_id", "pairs_weight",
"baseline", "feature_names")) {
if (!is.null(get(arg))) {
stop("parameter '", arg, "' should be NULL when the pool is read from file")
}
}
pool <- catboost.from_file(dataset, column_description, pairs, delimiter, has_header, thread_count)
} else if (is.matrix(dataset)) {
pool <- catboost.from_matrix(dataset, label, cat_features, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight,
baseline, feature_names)
} else if (is.dataset.frame(dataset)) {
pool <- catboost.from_data_frame(dataset, label, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight,
baseline, feature_names)
} else {
stop("Unsupported dataset type, expecting string, matrix or dafa.frame, got: ", class(dataset))
}
return(pool)
}
pool = catboost.load_pool(dataset, label = label_values, cat_features = c(0,3))
catboost.load_pool <- function(data, label = NULL, cat_features = NULL, column_description = NULL,
pairs = NULL, delimiter = "\t", has_header = FALSE, weight = NULL,
group_id = NULL, group_weight = NULL, subgroup_id = NULL, pairs_weight = NULL,
baseline = NULL, feature_names = NULL, thread_count = -1) {
if (!is.null(pairs) && (is.character(data) != is.character(pairs))) {
stop("Data and pairs should be the same types.")
}
if (is.character(data) && length(data) == 1) {
for (arg in list("label", "cat_features", "weight", "group_id",
"group_weight", "subgroup_id", "pairs_weight",
"baseline", "feature_names")) {
if (!is.null(get(arg))) {
stop("parameter '", arg, "' should be NULL when the pool is read from file")
}
}
pool <- catboost.from_file(data, column_description, pairs, delimiter, has_header, thread_count)
} else if (is.matrix(data)) {
pool <- catboost.from_matrix(data, label, cat_features, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight,
baseline, feature_names)
} else if (is.data.frame(data)) {
pool <- catboost.from_data_frame(data, label, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight,
baseline, feature_names)
} else {
stop("Unsupported data type, expecting string, matrix or dafa.frame, got: ", class(data))
}
return(pool)
}
pool = catboost.load_pool(dataset, label = label_values, cat_features = c(0,3))
##CATBOOST
data = treinando
glimpse(treinando)
label_values = c(0,1,1)
fit_params <- list(iterations = 100,
loss_function = 'Logloss',
ignored_features = c(4,9),
border_count = 32,
depth = 5,
learning_rate = 0.03,
l2_leaf_reg = 3.5,
task_type = 'GPU')
catboost.load_pool <- function(data, label = NULL, cat_features = NULL, column_description = NULL,
pairs = NULL, delimiter = "\t", has_header = FALSE, weight = NULL,
group_id = NULL, group_weight = NULL, subgroup_id = NULL, pairs_weight = NULL,
baseline = NULL, feature_names = NULL, thread_count = -1) {
if (!is.null(pairs) && (is.character(data) != is.character(pairs))) {
stop("Data and pairs should be the same types.")
}
if (is.character(data) && length(data) == 1) {
for (arg in list("label", "cat_features", "weight", "group_id",
"group_weight", "subgroup_id", "pairs_weight",
"baseline", "feature_names")) {
if (!is.null(get(arg))) {
stop("parameter '", arg, "' should be NULL when the pool is read from file")
}
}
pool <- catboost.from_file(data, column_description, pairs, delimiter, has_header, thread_count)
} else if (is.matrix(data)) {
pool <- catboost.from_matrix(data, label, cat_features, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight,
baseline, feature_names)
} else if (is.data.frame(data)) {
pool <- catboost.from_data_frame(data, label, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight,
baseline, feature_names)
} else {
stop("Unsupported data type, expecting string, matrix or dafa.frame, got: ", class(data))
}
return(pool)
}
pool = catboost.load_pool(dataset, label = label_values, cat_features = c(0,3))
##CATBOOST
dataset = treinando
glimpse(treinando)
label_values = c(0,1,1)
fit_params <- list(iterations = 100,
loss_function = 'Logloss',
ignored_features = c(4,9),
border_count = 32,
depth = 5,
learning_rate = 0.03,
l2_leaf_reg = 3.5,
task_type = 'GPU')
pool = catboost.load_pool(dataset, label = label_values, cat_features = c(0,3))
model <- catboost.train(pool, params = fit_params)
model <- catboost.train(dataset, params = fit_params)
